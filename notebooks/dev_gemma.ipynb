{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Set Transformers logging to error-only (no warnings or info)\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model_id = \"google/gemma-3-1b-pt\"\n",
    "target_model_id = \"google/gemma-3-12b-pt\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "max_memory = {0: \"5GiB\", 1: \"7GiB\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep draft model on single GPU\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    draft_model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    # quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(draft_model_id, use_fast=True)\n",
    "\n",
    "# Split target model across GPUs\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    target_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    # quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "k = 4\n",
    "prompt = \"Once upon a time\"\n",
    "max_tokens = 32\n",
    "use_cache = False\n",
    "\n",
    "# Set pad token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Pad prompt to align shape with compiled model\n",
    "input_ids = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=max_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ").to(draft_model.device)\n",
    "print(input_ids)\n",
    "cache = None\n",
    "\n",
    "for _ in range(4):\n",
    "    outputs = draft_model(\n",
    "        **input_ids,\n",
    "        past_key_values=None,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    cache = outputs.past_key_values\n",
    "    new_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdims=True)\n",
    "    ids = input_ids[\"input_ids\"]\n",
    "    ids[:] = torch.roll(ids, -1, dims=-1)\n",
    "    ids[:, -1] = new_token\n",
    "    attn = input_ids[\"attention_mask\"]\n",
    "    attn[:] = torch.roll(attn, -1, dims=-1)\n",
    "    attn[:, -1] = 1\n",
    "    print(input_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "k = 4\n",
    "prompt = \"Once upon a time\"\n",
    "max_tokens = 32\n",
    "use_cache = False\n",
    "\n",
    "# Set pad token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Determine how many tokens are in original prompt\n",
    "generated = tokenizer(prompt, return_tensors=\"pt\")\n",
    "N = generated[\"input_ids\"].shape[1]\n",
    "\n",
    "# Pad prompt to align shape with compiled model\n",
    "generated = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=max_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")[\"input_ids\"]\n",
    "\n",
    "cached_prompt = None\n",
    "successful_spec_decodes = 0\n",
    "draft_model_process_time = []\n",
    "target_model_process_time = []\n",
    "\n",
    "\n",
    "draft_model.eval()\n",
    "target_model.eval()\n",
    "with torch.inference_mode():\n",
    "    # Run until model is reached max number of tokens\n",
    "    print(\"\\n# Speculative Decoding Initiating #\")\n",
    "    while N < max_tokens:\n",
    "        print(f\"\\nSequence length: {N}\")\n",
    "        print(f\"Sequence text: '{tokenizer.decode(generated[0][-N+1:])}'\")\n",
    "        generated = generated.to(draft_model.device)\n",
    "        spec_sequence = generated[0].clone()\n",
    "        # Step 1: Generate draft output sequence of length k\n",
    "        spec_sequences = []\n",
    "        spec_tokens = []\n",
    "        start = time.perf_counter()\n",
    "        for i in range(k):\n",
    "            # Use generated sequence stored in spec_tokens\n",
    "            draft_logits = draft_model(spec_sequence.unsqueeze(0)).logits\n",
    "            # Greedy speculative decoding using argmax (for now..)\n",
    "            spec_token = draft_logits[:, -1, :].argmax(dim=-1)\n",
    "            spec_tokens.append(spec_token)\n",
    "            # Concatenate most recent speculative token to previous sequence\n",
    "            spec_sequence = torch.cat([spec_sequence[1:], spec_token])\n",
    "            spec_sequences.append({\"input_ids\": spec_sequence})\n",
    "        spec_tokens = torch.cat(spec_tokens)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Track draft model speed\n",
    "        draft_model_process_time.append(end - start)\n",
    "        print(f\"Draft Model: {k / (end - start):.3f} tok/s\")\n",
    "\n",
    "        # Step 2: Pad speculatize token sequences for use with target model\n",
    "        batched_input = tokenizer.pad(spec_sequences, return_tensors=\"pt\")\n",
    "\n",
    "        # Step 3: Process batched input\n",
    "        start = time.perf_counter()\n",
    "        if cached_prompt is None and use_cache:\n",
    "            # Cache prompt in target model\n",
    "            target_output = target_model(\n",
    "                **batched_input.to(target_model.device), use_cache=use_cache\n",
    "            )\n",
    "            target_logits = target_output.logits\n",
    "            cached_prompt = target_output.past_key_values\n",
    "        else:\n",
    "            # Decode using cached prompt\n",
    "            target_logits = target_model(\n",
    "                **batched_input.to(target_model.device),\n",
    "                past_key_values=cached_prompt,\n",
    "                use_cache=use_cache,\n",
    "            ).logits\n",
    "        target_logits = target_model(**batched_input.to(target_model.device)).logits\n",
    "        target_tokens = target_logits[:, -2, :].argmax(dim=-1)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Track target model speed\n",
    "        target_model_process_time.append(end - start)\n",
    "        print(f\"Target Model: {1 / (end - start):.3f} tok/s\")\n",
    "\n",
    "        # Step 4: Evaluate speculated tokens\n",
    "        matches = spec_tokens.to(target_model.device) == target_tokens\n",
    "        accepted_matches = matches.cumprod(dim=0).sum().item()\n",
    "\n",
    "        if accepted_matches > 0:\n",
    "            accepted_tokens = spec_tokens[:accepted_matches]\n",
    "        else:\n",
    "            # If no accepted tokens, use first token from target model\n",
    "            accepted_tokens = target_tokens[:1].to(draft_model.device)\n",
    "\n",
    "        # Step 5: Update generated sequence with accepted tokens\n",
    "        generated = torch.cat([generated, accepted_tokens[None, :]], dim=-1)\n",
    "        generated = generated[:, -max_tokens:]\n",
    "        N += len(accepted_tokens)\n",
    "        successful_spec_decodes += accepted_matches\n",
    "\n",
    "        print(f\"Accepted speculative tokens: {accepted_matches}\", end=\", \")\n",
    "        if accepted_matches > 0:\n",
    "            print(f\"Speculative Text: '{tokenizer.decode(accepted_tokens)}'\")\n",
    "        print(f\"SpecDecode: {len(accepted_tokens) / (end - start):.3f} tok/s\")\n",
    "\n",
    "# Print final sequence\n",
    "print(f\"\\nSequence length: {N}\")\n",
    "print(f\"Sequence text: {tokenizer.decode(generated[0][-N:])}\")\n",
    "print(\"\\n# Speculative Decoding Complete #\")\n",
    "draft_model_perf = (k * len(draft_model_process_time)) / sum(draft_model_process_time)\n",
    "print(f\"\\tDraft Model Perf: {draft_model_perf:.3f} tok/s\")\n",
    "target_model_perf = len(target_model_process_time) / sum(target_model_process_time)\n",
    "print(f\"\\tTarget Model Perf: {target_model_perf:.3f} tok/s\")\n",
    "spec_decode_perf = N / sum(target_model_process_time)\n",
    "print(f\"\\tSpec Decode Perf: {spec_decode_perf:.3f} tok/s\")\n",
    "print(f\"\\tSpec Decode Speedup: {spec_decode_perf / target_model_perf:.1f}\")\n",
    "acceptance_ratio = successful_spec_decodes / (k * max_tokens + 1e-9)\n",
    "print(f\"\\tSpec Decode Acceptance Ratio: {acceptance_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe088b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
