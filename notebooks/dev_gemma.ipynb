{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fc84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Set Transformers logging to error-only (no warnings or info)\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model_id = \"google/gemma-3-1b-pt\"\n",
    "target_model_id = \"google/gemma-3-12b-pt\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "max_memory = {0: \"5GiB\", 1: \"7GiB\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf96ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714cd26f1eca44b89f059157fb99a3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "# Keep draft model on single GPU\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    draft_model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    # quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(draft_model_id, use_fast=True)\n",
    "\n",
    "# Split target model across GPUs\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    target_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_memory=max_memory,\n",
    "    # quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7b43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Speculative Decoding Initiating #\n",
      "\n",
      "Sequence length: 5\n",
      "Sequence text: 'Once upon a time'\n",
      "Draft Model: 14.320 tok/s\n",
      "Target Model: 0.228 tok/s\n",
      "Accepted speculative tokens: 4, Speculative Text: ', there was a'\n",
      "SpecDecode: 0.912 tok/s\n",
      "\n",
      "Sequence length: 9\n",
      "Sequence text: 'Once upon a time, there was a'\n",
      "Draft Model: 40.196 tok/s\n",
      "Target Model: 0.231 tok/s\n",
      "Accepted speculative tokens: 2, Speculative Text: ' little girl'\n",
      "SpecDecode: 0.462 tok/s\n",
      "\n",
      "Sequence length: 11\n",
      "Sequence text: 'Once upon a time, there was a little girl'\n",
      "Draft Model: 40.575 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.236 tok/s\n",
      "\n",
      "Sequence length: 12\n",
      "Sequence text: 'Once upon a time, there was a little girl who'\n",
      "Draft Model: 40.705 tok/s\n",
      "Target Model: 0.235 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.235 tok/s\n",
      "\n",
      "Sequence length: 13\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved'\n",
      "Draft Model: 40.855 tok/s\n",
      "Target Model: 0.234 tok/s\n",
      "Accepted speculative tokens: 4, Speculative Text: ' to read. She'\n",
      "SpecDecode: 0.936 tok/s\n",
      "\n",
      "Sequence length: 17\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She'\n",
      "Draft Model: 40.741 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.236 tok/s\n",
      "\n",
      "Sequence length: 18\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would'\n",
      "Draft Model: 40.936 tok/s\n",
      "Target Model: 0.233 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.233 tok/s\n",
      "\n",
      "Sequence length: 19\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read'\n",
      "Draft Model: 39.501 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.236 tok/s\n",
      "\n",
      "Sequence length: 20\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read anything'\n",
      "Draft Model: 39.892 tok/s\n",
      "Target Model: 0.235 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.235 tok/s\n",
      "\n",
      "Sequence length: 21\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read anything she'\n",
      "Draft Model: 40.491 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 4, Speculative Text: ' could get her hands'\n",
      "SpecDecode: 0.942 tok/s\n",
      "\n",
      "Sequence length: 25\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read anything she could get her hands'\n",
      "Draft Model: 40.473 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 3, Speculative Text: ' on, from'\n",
      "SpecDecode: 0.709 tok/s\n",
      "\n",
      "Sequence length: 28\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read anything she could get her hands on, from'\n",
      "Draft Model: 40.536 tok/s\n",
      "Target Model: 0.232 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.232 tok/s\n",
      "\n",
      "Sequence length: 29\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read anything she could get her hands on, from picture'\n",
      "Draft Model: 40.544 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 2, Speculative Text: ' books to'\n",
      "SpecDecode: 0.472 tok/s\n",
      "\n",
      "Sequence length: 31\n",
      "Sequence text: 'Once upon a time, there was a little girl who loved to read. She would read anything she could get her hands on, from picture books to'\n",
      "Draft Model: 40.586 tok/s\n",
      "Target Model: 0.236 tok/s\n",
      "Accepted speculative tokens: 0, SpecDecode: 0.236 tok/s\n",
      "\n",
      "Sequence length: 32\n",
      "Sequence text: <bos>Once upon a time, there was a little girl who loved to read. She would read anything she could get her hands on, from picture books to chapter\n",
      "\n",
      "# Speculative Decoding Complete #\n",
      "\tDraft Model Perf: 35.793 tok/s\n",
      "\tTarget Model Perf: 0.234 tok/s\n",
      "\tSpec Decode Perf: 0.536 tok/s\n",
      "\tSpec Decode Speedup: 2.3\n",
      "\tSpec Decode Acceptance Ratio: 0.148\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "k = 4\n",
    "prompt = \"Once upon a time\"\n",
    "max_tokens = 32\n",
    "use_cache = False\n",
    "\n",
    "# Set pad token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Determine how many tokens are in original prompt\n",
    "generated = tokenizer(prompt, return_tensors=\"pt\")\n",
    "N = generated[\"input_ids\"].shape[1]\n",
    "\n",
    "# Pad prompt to align shape with compiled model\n",
    "generated = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=max_tokens,\n",
    "    return_tensors=\"pt\",\n",
    ")[\"input_ids\"]\n",
    "\n",
    "cached_prompt = None\n",
    "successful_spec_decodes = 0\n",
    "draft_model_process_time = []\n",
    "target_model_process_time = []\n",
    "\n",
    "\n",
    "draft_model.eval()\n",
    "target_model.eval()\n",
    "with torch.inference_mode():\n",
    "    # Run until model is reached max number of tokens\n",
    "    print(\"\\n# Speculative Decoding Initiating #\")\n",
    "    while N < max_tokens:\n",
    "        print(f\"\\nSequence length: {N}\")\n",
    "        print(f\"Sequence text: '{tokenizer.decode(generated[0][-N+1:])}'\")\n",
    "        generated = generated.to(draft_model.device)\n",
    "        spec_sequence = generated[0].clone()\n",
    "        # Step 1: Generate draft output sequence of length k\n",
    "        spec_sequences = []\n",
    "        spec_tokens = []\n",
    "        start = time.perf_counter()\n",
    "        for i in range(k):\n",
    "            # Use generated sequence stored in spec_tokens\n",
    "            draft_logits = draft_model(spec_sequence.unsqueeze(0)).logits\n",
    "            # Greedy speculative decoding using argmax (for now..)\n",
    "            spec_token = draft_logits[:, -1, :].argmax(dim=-1)\n",
    "            spec_tokens.append(spec_token)\n",
    "            # Concatenate most recent speculative token to previous sequence\n",
    "            spec_sequence = torch.cat([spec_sequence[1:], spec_token])\n",
    "            spec_sequences.append({\"input_ids\": spec_sequence})\n",
    "        spec_tokens = torch.cat(spec_tokens)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Track draft model speed\n",
    "        draft_model_process_time.append(end - start)\n",
    "        print(f\"Draft Model: {k / (end - start):.3f} tok/s\")\n",
    "\n",
    "        # Step 2: Pad speculatize token sequences for use with target model\n",
    "        batched_input = tokenizer.pad(spec_sequences, return_tensors=\"pt\")\n",
    "\n",
    "        # Step 3: Process batched input\n",
    "        start = time.perf_counter()\n",
    "        if cached_prompt is None and use_cache:\n",
    "            # Cache prompt in target model\n",
    "            target_output = target_model(\n",
    "                **batched_input.to(target_model.device), use_cache=use_cache\n",
    "            )\n",
    "            target_logits = target_output.logits\n",
    "            cached_prompt = target_output.past_key_values\n",
    "        else:\n",
    "            # Decode using cached prompt\n",
    "            target_logits = target_model(\n",
    "                **batched_input.to(target_model.device),\n",
    "                past_key_values=cached_prompt,\n",
    "                use_cache=use_cache,\n",
    "            ).logits\n",
    "        target_logits = target_model(**batched_input.to(target_model.device)).logits\n",
    "        target_tokens = target_logits[:, -2, :].argmax(dim=-1)\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Track target model speed\n",
    "        target_model_process_time.append(end - start)\n",
    "        print(f\"Target Model: {1 / (end - start):.3f} tok/s\")\n",
    "\n",
    "        # Step 4: Evaluate speculated tokens\n",
    "        matches = spec_tokens.to(target_model.device) == target_tokens\n",
    "        accepted_matches = matches.cumprod(dim=0).sum().item()\n",
    "\n",
    "        if accepted_matches > 0:\n",
    "            accepted_tokens = spec_tokens[:accepted_matches]\n",
    "        else:\n",
    "            # If no accepted tokens, use first token from target model\n",
    "            accepted_tokens = target_tokens[:1].to(draft_model.device)\n",
    "\n",
    "        # Step 5: Update generated sequence with accepted tokens\n",
    "        generated = torch.cat([generated, accepted_tokens[None, :]], dim=-1)\n",
    "        generated = generated[:, -max_tokens:]\n",
    "        N += len(accepted_tokens)\n",
    "        successful_spec_decodes += accepted_matches\n",
    "\n",
    "        print(f\"Accepted speculative tokens: {accepted_matches}\", end=\", \")\n",
    "        if accepted_matches > 0:\n",
    "            print(f\"Speculative Text: '{tokenizer.decode(accepted_tokens)}'\")\n",
    "        print(f\"SpecDecode: {len(accepted_tokens) / (end - start):.3f} tok/s\")\n",
    "\n",
    "# Print final sequence\n",
    "print(f\"\\nSequence length: {N}\")\n",
    "print(f\"Sequence text: {tokenizer.decode(generated[0][-N:])}\")\n",
    "print(\"\\n# Speculative Decoding Complete #\")\n",
    "draft_model_perf = (k * len(draft_model_process_time)) / sum(draft_model_process_time)\n",
    "print(f\"\\tDraft Model Perf: {draft_model_perf:.3f} tok/s\")\n",
    "target_model_perf = len(target_model_process_time) / sum(target_model_process_time)\n",
    "print(f\"\\tTarget Model Perf: {target_model_perf:.3f} tok/s\")\n",
    "spec_decode_perf = N / sum(target_model_process_time)\n",
    "print(f\"\\tSpec Decode Perf: {spec_decode_perf:.3f} tok/s\")\n",
    "print(f\"\\tSpec Decode Speedup: {spec_decode_perf / target_model_perf:.1f}\")\n",
    "acceptance_ratio = successful_spec_decodes / (k * max_tokens + 1e-9)\n",
    "print(f\"\\tSpec Decode Acceptance Ratio: {acceptance_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe088b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
