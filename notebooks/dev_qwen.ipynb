{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fc84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "target_model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf96ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75b8745cdc948c189940c6719662cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep draft model on single GPU\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    draft_model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(draft_model_id)\n",
    "\n",
    "# Split target model across GPUs\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    target_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f1b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, tokenizer, max_prompt_len):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_len,\n",
    "        enable_thinking=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8aeec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_prompt_len, max_new_tokens):\n",
    "    with torch.inference_mode():\n",
    "        inputs = prepare_prompt(prompt, tokenizer, max_prompt_len)\n",
    "        outputs = model.generate(\n",
    "            **inputs.to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    # return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1] :])\n",
    "    return outputs[0][inputs[\"input_ids\"].shape[-1] :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbed143",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_len = 512\n",
    "max_new_tokens = 64\n",
    "prompt = \"Concisely, what is speculative decoding in relation to LLM performance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.666312 tok/sec\n",
      "Speculative decoding refers to a technique used in language models (LLMs) to enhance their performance by making assumptions about the next word or context based on prior information. This approach allows the model to make predictions more accurately, especially when the context is incomplete or uncertain, thereby improving the fluency and coherence of the generated text\n"
     ]
    }
   ],
   "source": [
    "# Draft Model\n",
    "start = time.perf_counter()\n",
    "output = generate(draft_model, tokenizer, prompt, max_prompt_len, max_new_tokens)\n",
    "end = time.perf_counter()\n",
    "print(f\"{len(output) / (end - start):.1f} tok/sec\")\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.615868 tok/sec\n",
      "Speculative decoding is a technique used to accelerate the inference of large language models (LLMs) by using a smaller, faster model to generate text speculatively in parallel with the main model, improving throughput without sacrificing accuracy.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Target Model\n",
    "start = time.perf_counter()\n",
    "output = generate(target_model, tokenizer, prompt, max_prompt_len, max_new_tokens)\n",
    "end = time.perf_counter()\n",
    "print(f\"{len(output) / (end - start):.1f} tok/sec\")\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b43e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
