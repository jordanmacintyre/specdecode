{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fc84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "target_model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf96ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0d392e61fc491fa8e7ba8e3c37548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep draft model on single GPU\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    draft_model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(draft_model_id)\n",
    "\n",
    "# Split target model across GPUs\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    target_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f1b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(prompt, tokenizer, max_prompt_len):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_len,\n",
    "        enable_thinking=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8aeec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_prompt_len, max_new_tokens):\n",
    "    with torch.inference_mode():\n",
    "        inputs = prepare_prompt(prompt, tokenizer, max_prompt_len)\n",
    "        outputs = model.generate(\n",
    "            **inputs.to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1] :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbed143",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_len = 512\n",
    "max_new_tokens = 64\n",
    "prompt = \"Concisely, what is speculative decoding in relation to LLM performance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbd83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculative decoding is a technique used in language models to enhance their performance by making predictions about future text or data based on the current context or previous information. This approach allows models to generate more accurate and contextually relevant responses, improving their ability to understand and respond to complex or ambiguous situations. It is often applied in tasks\n"
     ]
    }
   ],
   "source": [
    "# Draft Model\n",
    "result = generate(draft_model, tokenizer, prompt, max_prompt_len, max_new_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb63e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculative decoding is a technique used to accelerate large language model (LLM) inference by generating text in parallel using a faster, smaller model (the \"speculator\") while the main model processes the input sequentially. This improves latency without sacrificing accuracy, as the main model can later verify and correct the speculator's output\n"
     ]
    }
   ],
   "source": [
    "# Target Model\n",
    "result = generate(target_model, tokenizer, prompt, max_prompt_len, max_new_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b43e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
