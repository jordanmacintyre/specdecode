# Model Configs
draft:
  quantized: false
  params:
    pretrained_model_name_or_path: "google/gemma-3-1b-pt"
    device_map: "cuda:0"
    torch_dtype: "auto"

target: 
  quantized: false
  params:
    pretrained_model_name_or_path: "google/gemma-3-12b-pt"
    device_map: "cpu"
    torch_dtype: "auto"
    
# Generation Configs
prompt: "Q: Where is the largets city in the world? A:"
max_length: 64
max_new_tokens: 32
k: 2
summary_file_name: "gemma-3-d1b-t12b-pt" 
